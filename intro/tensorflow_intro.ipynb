{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Intro to TensorFlow\n",
    "\n",
    "The following is a small intro to how computational frameworks (theano, tensorflow) work in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The MNIST dataset\n",
    "\n",
    "It's just a set of 28 x 28 pixels of hand-written digits (greyscale) with labels which digit is on the picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# first we need some data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Let's load a minibatch of 100\n",
    "\n",
    "We'll provide such mini-batches to our model during training to show it what it should learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_samples, batch_labels = mnist.train.next_batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 784)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Now let's check an actual image:\n",
    "\n",
    "Images are represented as a vector of number values, corresponding to the color intensity. Any sort of data can be represented as **numbers**, i.e. **vectors**. \n",
    "\n",
    "This means you can think of any sort of object as a point in a very high-dimensional space. Example: if your object is characterized by **(age, height, income)**, it's a point in 3D space. Our images above are points in 784-dim space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.5529412 ,\n",
       "        1.        ,  0.50196081,  0.09803922,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.07450981,  0.39607847,  0.94117653,  0.99215692,  0.98823535,\n",
       "        0.70588237,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.34509805,  0.98823535,\n",
       "        0.98823535,  0.99215692,  0.98823535,  0.76470596,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.03921569,  0.7960785 ,  0.98823535,  0.98823535,  0.99215692,\n",
       "        0.88627458,  0.37647063,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.63921571,  0.98823535,\n",
       "        0.98823535,  0.98823535,  0.74509805,  0.04705883,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.20000002,  0.9333334 ,  0.99215692,  0.99215692,  0.74509805,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.9333334 ,  0.98823535,\n",
       "        0.98823535,  0.98823535,  0.43921572,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.43529415,  0.99215692,  0.98823535,  0.98823535,  0.77647066,\n",
       "        0.09803922,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.07450981,  0.84313732,  0.99215692,\n",
       "        0.98823535,  0.83921576,  0.10980393,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.41960788,  0.98823535,  0.99215692,  0.98823535,  0.46274513,\n",
       "        0.30980393,  0.30980393,  0.5529412 ,  0.54901963,  0.54901963,\n",
       "        0.54901963,  0.24705884,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.22352943,  0.99215692,  0.99215692,\n",
       "        1.        ,  0.99215692,  0.99215692,  0.99215692,  0.99215692,\n",
       "        1.        ,  0.99215692,  0.99215692,  0.99215692,  0.99215692,\n",
       "        0.75294125,  0.04705883,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.22352943,  0.98823535,  0.98823535,  0.99215692,  0.98823535,\n",
       "        0.98823535,  0.98823535,  0.98823535,  0.99215692,  0.98823535,\n",
       "        0.98823535,  0.98823535,  0.98823535,  0.99215692,  0.70980394,\n",
       "        0.07450981,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.22352943,  0.98823535,\n",
       "        0.98823535,  0.99215692,  0.98823535,  0.98823535,  0.98823535,\n",
       "        0.86274517,  0.76862752,  0.46274513,  0.81568635,  0.86274517,\n",
       "        0.98823535,  0.99215692,  0.98823535,  0.21568629,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.58823532,  0.98823535,  0.98823535,  0.99215692,\n",
       "        0.98823535,  0.77647066,  0.32941177,  0.14509805,  0.        ,\n",
       "        0.        ,  0.09803922,  0.31764707,  0.98823535,  0.99215692,\n",
       "        0.98823535,  0.21568629,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.77254909,\n",
       "        0.98823535,  0.98823535,  0.99215692,  0.80000007,  0.09803922,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.22352943,\n",
       "        0.98823535,  0.98823535,  0.99215692,  0.80000007,  0.09803922,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.22352943,  0.99215692,  0.99215692,\n",
       "        1.        ,  0.35294119,  0.        ,  0.        ,  0.24705884,\n",
       "        0.44705886,  0.9333334 ,  0.99215692,  0.99215692,  0.99215692,\n",
       "        0.80000007,  0.05882353,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.02352941,  0.69411767,  0.98823535,  0.99215692,  0.95294124,\n",
       "        0.88235301,  0.88235301,  0.94117653,  0.99215692,  0.98823535,\n",
       "        0.98823535,  0.94901967,  0.34901962,  0.05882353,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.53725493,\n",
       "        0.98823535,  0.99215692,  0.98823535,  0.98823535,  0.98823535,\n",
       "        0.98823535,  0.99215692,  0.78823537,  0.64313728,  0.19215688,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.03921569,  0.63137257,  0.99215692,\n",
       "        0.98823535,  0.98823535,  0.97647065,  0.56862748,  0.32941177,\n",
       "        0.03529412,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.43921572,  0.61960787,  0.5529412 ,\n",
       "        0.38823533,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = batch_samples[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f70738354a8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7073cbe0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = image.reshape((28, 28))\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Let's check what a label is\n",
    "\n",
    "When we do classification, we use 1-hot encodings to represent the labels for multiple classes. Example, if there are 5 different classes and a point is in class 4:\n",
    "\n",
    "```\n",
    "[0, 0, 0, 1, 0]\n",
    "```\n",
    "\n",
    "Is the one-hot encoding of this class-membership. \n",
    "\n",
    "We use 1-hot encodings because if we just used numbers: 1 for class 1, 2 for class 2, etc... then the network would understand that class 4 is *\"4 times bigger\"* than class 1, which makes no sense and will lead the network in the wrong direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = batch_labels[0]\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Now let's classify those digits in TensorFlow\n",
    "\n",
    "Tensorflow is a computational framework, it let's us define mathematical expressions (i.e. formulas) and execute them on some inputs (distributed on GPUs even).\n",
    "\n",
    "It also provides automatic gradient computations (differentiation of the formulas).\n",
    "\n",
    "The formulas are first defined **symbolically**, as a template, and then used on actual data. Just like one does in math: you have a function with some parameters and inputs (the symbolic variables) and when you evaluate the function you plug in real values into those.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### First, let us construct our mapping from data to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# define a symbolic variable for the image input\n",
    "# None is special syntax in most deep learning frameworks,\n",
    "# it means this dimension might vary (it's the batch size)\n",
    "x = tf.placeholder(dtype=tf.float32, \n",
    "                   shape=[None, 784])\n",
    "\n",
    "# define symbolic variables for the parameters of our function\n",
    "# tf.Variables are like function parameters, they are not the \n",
    "# arguments of the function, they define what the function\n",
    "W = tf.Variable(tf.zeros(shape=[784, 10]))\n",
    "b = tf.Variable(tf.zeros(shape=[10]))\n",
    "\n",
    "# Now we will construct our function expression:\n",
    "#\n",
    "# Wx + b\n",
    "#\n",
    "# It's a linear transformation of the input\n",
    "preds = tf.matmul(x, W) + b\n",
    "\n",
    "# We now have transformed x to a 10-dim. vector,\n",
    "# but we want to interpret each output as the probability\n",
    "# of each of the 10 classes: and discrete probabilities sum up to 1.\n",
    "# So we apply a softmax() to make this happen:\n",
    "preds = tf.nn.softmax(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Now let's define our loss (what we need to optimize)\n",
    "\n",
    "The loss for multi-class classification is the [categorical cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy).\n",
    "\n",
    "When we define prediction & classification losses, we always need the true labels (ground truths) to *\"compare\"* against:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# this is now an argument (input) of the cost function,\n",
    "# and not a parameter - hence tf.placeholder\n",
    "targets = tf.placeholder(dtype=tf.float32, shape=[None, 10])\n",
    "\n",
    "# now we define the cross_entropy formula\n",
    "temp = - targets * tf.log(preds)\n",
    "\n",
    "# but we need to sum over axis 1 (0 index),\n",
    "# that's the 10-dim axis\n",
    "temp = tf.reduce_sum(temp, reduction_indices=[1])\n",
    "\n",
    "# and then take the mean over all samples in the batch\n",
    "cross_entropy_loss = tf.reduce_mean(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Now we need to optimize the loss (minimize it)\n",
    "\n",
    "We use a gradient descent method and gradually step down the loss surface until we reach a local minimum.\n",
    "\n",
    "```\n",
    "params = params - learning_rate * gradient\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# we define our optimization method\n",
    "# it will automatically do the derivatives for us\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5)\n",
    "\n",
    "# and then we take the update step:\n",
    "#\n",
    "# params = params - learning_rate * gradient\n",
    "#\n",
    "# from it\n",
    "# NOTE: this is still a symbolic !!\n",
    "update_step = optimizer.minimize(loss=cross_entropy_loss,\n",
    "                                var_list=[W, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Finally, let's do the actual optimization\n",
    "\n",
    "Now we get to plug in real data in the above template, everything so far has been just (symbolic) declaration of what we would want to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# we need a session to actually run expressions\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# we initialize all global variables\n",
    "# those are W and b in this case\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# and now we do 1000 optimizations\n",
    "for _ in range(1000):\n",
    "    batch_samples, batch_labels = mnist.train.next_batch(100)\n",
    "    \n",
    "    # we run the update_step from above, feeding in actual values\n",
    "    # for x and targets\n",
    "    sess.run(update_step, feed_dict={x: batch_samples,\n",
    "                                    targets: batch_labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Let's see how our model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91659999"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we define a symbolic expression to tell if a prediction is\n",
    "# correct or not\n",
    "\n",
    "# we need the argmax because preds is between 0 and 1, not\n",
    "# only 0 and 1\n",
    "correct_prediction = tf.equal(tf.argmax(preds,1), tf.argmax(targets,1))\n",
    "\n",
    "# accuracy is the percentage of correct predictions over all\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "# and now we execute the above symbolic expression on the actual data\n",
    "# NOTE: we use a test set to check the accuracy, which the model has not\n",
    "# seen before\n",
    "sess.run(accuracy, feed_dict={x: mnist.test.images, \n",
    "                            targets: mnist.test.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### ~ 92 % accurate predictions with a simple logistic regression, not too shabby :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
